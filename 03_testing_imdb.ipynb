{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_testing_imdb.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1trSGbA3Db6L"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK6K96eNDBci"
      },
      "source": [
        "# Teste final com o dataset IMDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pb3mBS4ueM3"
      },
      "source": [
        "!pip install ktrain\n",
        "import ktrain\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaZrsQr7uBH9",
        "outputId": "74506b0c-3862-44e3-b560-316c7621d714"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU8wy3OzuOlT"
      },
      "source": [
        "# Pegando o dataset original que estamos trabalhando (Pelo Google Drive)\n",
        "df = pd.read_csv('gdrive/MyDrive/imdb_cleaned.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RTlX3m8yukfW",
        "outputId": "334eb585-1370-42ab-b94c-1a975810ea1e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>cleaner</th>\n",
              "      <th>cleaner_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>['one', 'review', 'mention', 'watch', 'oz', 'e...</td>\n",
              "      <td>one review mention watch oz episod hook right ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A wonderful little production.     The filming...</td>\n",
              "      <td>positive</td>\n",
              "      <td>['wonder', 'littl', 'product', 'film', 'techni...</td>\n",
              "      <td>wonder littl product film techniqu fashion giv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "      <td>['thought', 'wonder', 'way', 'spend', 'time', ...</td>\n",
              "      <td>thought wonder way spend time hot summer weeke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>['basic', 'famili', 'littl', 'boy', 'jake', 't...</td>\n",
              "      <td>basic famili littl boy jake think zombi closet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "      <td>['petter', 'mattei', 'love', 'time', 'money', ...</td>\n",
              "      <td>petter mattei love time money visual stun film...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                        cleaner_str\n",
              "0           0  ...  one review mention watch oz episod hook right ...\n",
              "1           1  ...  wonder littl product film techniqu fashion giv...\n",
              "2           2  ...  thought wonder way spend time hot summer weeke...\n",
              "3           3  ...  basic famili littl boy jake think zombi closet...\n",
              "4           4  ...  petter mattei love time money visual stun film...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow2UqBBOUqK9"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6dXDuAHVrq8"
      },
      "source": [
        "# Função pra calcular algumas métricas dado os dados esperados e os dados previstos\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    print(\"Accuracy: \" + str((tp + tn) / (tp + tn + fp + fn)) )\n",
        "    print(\"Precision: \" + str(tp / (tp + fp)) )\n",
        "    print(\"Recall: \" + str(tp / (tp + fn)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAjIN4gQDmQW"
      },
      "source": [
        "## sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VHv_rBrDosI"
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Classificadores para testar\n",
        "experimentos = {\n",
        "    \"Perceptron 0.0001\": Perceptron(),\n",
        "    \"Random Forest 400 ent\": RandomForestClassifier(n_estimators=400, criterion='entropy'),\n",
        "    \"GradientBoost 400 0.5\": GradientBoostingClassifier(n_estimators=400, learning_rate=0.5),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7XgNFsFDxvu"
      },
      "source": [
        "# Preparando o vetorizador TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer3 = TfidfVectorizer(max_df=0.9, min_df=100, ngram_range=(1,2))\n",
        "X3 = vectorizer3.fit_transform(df['cleaner_str'])\n",
        "y = df['sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRNda_gVu4xO"
      },
      "source": [
        "#TF-IDF2 (bigramas, min menor)\n",
        "vectorizer4 = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=5)\n",
        "X4 = vectorizer4.fit_transform(df['cleaner_str'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9018I0PHTD0",
        "outputId": "28b63f00-6f41-4116-f4f3-2b6e77fafac8"
      },
      "source": [
        "# Treinando os modelos com todos os dados\n",
        "clf_pp = experimentos['Perceptron 0.0001']\n",
        "print('Fitting '+ type(clf_pp).__name__)\n",
        "clf_pp.fit(X3, y)\n",
        "\n",
        "clf_rf = experimentos['Random Forest 400 ent']\n",
        "print('Fitting '+ type(clf_rf).__name__)\n",
        "clf_rf.fit(X3, y)\n",
        "\n",
        "clf_gb = experimentos['GradientBoost 400 0.5']\n",
        "print('Fitting '+ type(clf_gb).__name__)\n",
        "clf_gb.fit(X3, y)\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting Perceptron\n",
            "Fitting RandomForestClassifier\n",
            "Fitting GradientBoostingClassifier\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTRjfuRlvOPj",
        "outputId": "a1f4db77-9496-4f47-bb72-e19c95ac4c7a"
      },
      "source": [
        "# Treinando os modelos com todos os dados (TF-IDF2)\n",
        "clf_pp2 = experimentos['Perceptron 0.0001']\n",
        "print('Fitting '+ type(clf_pp2).__name__)\n",
        "clf_pp2.fit(X4, y)\n",
        "\n",
        "clf_rf2 = experimentos['Random Forest 400 ent']\n",
        "print('Fitting '+ type(clf_rf2).__name__)\n",
        "clf_rf2.fit(X4, y)\n",
        "\n",
        "clf_gb2 = experimentos['GradientBoost 400 0.5']\n",
        "print('Fitting '+ type(clf_gb2).__name__)\n",
        "clf_gb2.fit(X4, y)\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting Perceptron\n",
            "Fitting RandomForestClassifier\n",
            "Fitting GradientBoostingClassifier\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGg-eTiiTBPI",
        "outputId": "8f223973-da2d-4271-b976-708c61c1a5e1"
      },
      "source": [
        "test = df.sample(1000)\n",
        "X_test = vectorizer3.transform(test['cleaner_str'])\n",
        "y_test = test['sentiment']\n",
        "calculate_metrics(y_test, clf_pp.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.92\n",
            "Precision: 0.9489795918367347\n",
            "Recall: 0.8942307692307693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2N0hGIn5fjo",
        "outputId": "74220d35-f0c0-4cbd-b5ff-b5149ea846b6"
      },
      "source": [
        "test = df.sample(1000)\n",
        "X_test = vectorizer4.transform(test['cleaner_str'])\n",
        "y_test = test['sentiment']\n",
        "calculate_metrics(y_test, clf_pp2.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.999\n",
            "Precision: 0.998\n",
            "Recall: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1trSGbA3Db6L"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vZCqe6WunTX",
        "outputId": "23128285-ee1c-4501-98b1-b85aa637cc06"
      },
      "source": [
        "!unzip gdrive/MyDrive/bert.zip #Pegando os arquivos obtidos pelo learner no último notebook (Pelo Google Drive)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  gdrive/MyDrive/bert.zip\n",
            "   creating: content/bert/\n",
            "  inflating: content/bert/tf_model.h5  \n",
            "  inflating: content/bert/tf_model.preproc  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgPEs66oRPVc"
      },
      "source": [
        "# Carregando o modelo BERT treinado\n",
        "predictor_load = ktrain.load_predictor('content/bert')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK6BRaJFS8bq",
        "outputId": "1004901b-f7e6-411b-d436-01391531d120"
      },
      "source": [
        "#Teste inicial pra ver se funcionou\n",
        "data = ['this movie was horrible, the plot was really boring. acting was okay',\n",
        "        'the fild is really sucked. there is not plot and acting was bad',\n",
        "        'what a beautiful movie. great plot. acting was good. will see it again']\n",
        "\n",
        "predicted = predictor_load.predict(data)\n",
        "predicted"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['negative', 'negative', 'positive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY_tVKSGVgxB",
        "outputId": "d072092e-3021-4807-dc2a-c2989354208e"
      },
      "source": [
        "# Pegando um sample do dataset original e testando a performance do modelo BERT\n",
        "sample = df.sample(1000, random_state=0)\n",
        "expected = sample.sentiment.values\n",
        "predicted = predictor_load.predict(sample.review.values)\n",
        "calculate_metrics(expected, predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.994\n",
            "Precision: 0.9898580121703854\n",
            "Recall: 0.9979550102249489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt8oumywY3eh"
      },
      "source": [
        "print(expected)\n",
        "print(predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByyJJ3E3cqlq"
      },
      "source": [
        "# Testando eficácia com outros conjuntos de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzOVhbpu_ZTJ"
      },
      "source": [
        "# Importações pra pre-processar o texto\n",
        "\n",
        "import nltk\n",
        "from nltk import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import string\n",
        "punc = string.punctuation\n",
        "\n",
        "!pip install contractions\n",
        "import contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS-orl7X92F9"
      },
      "source": [
        "# Função pra executar as transformações no texto bruto\n",
        "\n",
        "def transform_text(raw_df, raw_text_column, class_column):\n",
        "  df = raw_df.copy()\n",
        "  df['no_contract'] = df[raw_text_column].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
        "  df['review_description_str'] = [' '.join(map(str, l)) for l in df['no_contract']]\n",
        "  df['tokenized'] = df['review_description_str'].apply(word_tokenize)\n",
        "  df['lower'] = df['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
        "  df['no_punc'] = df['lower'].apply(lambda x: [word for word in x if word not in punc])\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  df['stopwords_removed'] = df['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "  stemmer = SnowballStemmer(language='english')\n",
        "  df['stemmed'] = df['stopwords_removed'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "  df['cleaner'] = df['stemmed'].apply(lambda x: [word for word in x if word.isalpha()])\n",
        "  df['cleaner_str'] = [' '.join(map(str,l)) for l in df['cleaner']]\n",
        "\n",
        "  return df[[raw_text_column, class_column, 'cleaner', 'cleaner_str']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLqMo1TEQt3m"
      },
      "source": [
        "# Função pra calcular as métricas de cada um dos três classificadores\n",
        "def test_models(X, y, name):\n",
        "  print(\"Testing model Perceptron with \"+ name + \" dataset\")\n",
        "  calculate_metrics(y, clf_pp.predict(X))\n",
        "  print(\"Testing model Random Forest with \"+ name + \" dataset\")\n",
        "  calculate_metrics(y, clf_rf.predict(X))\n",
        "  print(\"Testing model Gradient Boost with \"+ name + \" dataset\")\n",
        "  calculate_metrics(y, clf_gb.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVkgU07A6qYA"
      },
      "source": [
        "# Função pra calcular as métricas de cada um dos três classificadores (2)\n",
        "def test_models2(X, y, name):\n",
        "  print(\"Testing model Perceptron with \"+ name + \" dataset\")\n",
        "  calculate_metrics(y, clf_pp2.predict(X))\n",
        "  print(\"Testing model Random Forest with \"+ name + \" dataset\")\n",
        "  calculate_metrics(y, clf_rf2.predict(X))\n",
        "  print(\"Testing model Gradient Boost with \"+ name + \" dataset\")\n",
        "  calculate_metrics(y, clf_gb2.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDKH1TvVZ6Ww"
      },
      "source": [
        "## Rotten Tomatoes\n",
        "Rotten Tomatoes é um agregador de críticas de cinema e televisão. Parecido com o IMDB, porém as reviews no RT só são feitas por críticos especializados de algum veículo de mídia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "8P3Rs4Jzdt0S",
        "outputId": "b90ccf46-c5e2-4463-f672-4903acdac59c"
      },
      "source": [
        "df_rt = pd.read_csv('gdrive/MyDrive/rotten_tomatoes_critic_reviews.csv')\n",
        "df_rt.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rotten_tomatoes_link</th>\n",
              "      <th>critic_name</th>\n",
              "      <th>top_critic</th>\n",
              "      <th>publisher_name</th>\n",
              "      <th>review_type</th>\n",
              "      <th>review_score</th>\n",
              "      <th>review_date</th>\n",
              "      <th>review_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>m/0814255</td>\n",
              "      <td>Andrew L. Urban</td>\n",
              "      <td>False</td>\n",
              "      <td>Urban Cinefile</td>\n",
              "      <td>Fresh</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2010-02-06</td>\n",
              "      <td>A fantasy adventure that fuses Greek mythology...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>m/0814255</td>\n",
              "      <td>Louise Keller</td>\n",
              "      <td>False</td>\n",
              "      <td>Urban Cinefile</td>\n",
              "      <td>Fresh</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2010-02-06</td>\n",
              "      <td>Uma Thurman as Medusa, the gorgon with a coiff...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>m/0814255</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>FILMINK (Australia)</td>\n",
              "      <td>Fresh</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2010-02-09</td>\n",
              "      <td>With a top-notch cast and dazzling special eff...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>m/0814255</td>\n",
              "      <td>Ben McEachen</td>\n",
              "      <td>False</td>\n",
              "      <td>Sunday Mail (Australia)</td>\n",
              "      <td>Fresh</td>\n",
              "      <td>3.5/5</td>\n",
              "      <td>2010-02-09</td>\n",
              "      <td>Whether audiences will get behind The Lightnin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>m/0814255</td>\n",
              "      <td>Ethan Alter</td>\n",
              "      <td>True</td>\n",
              "      <td>Hollywood Reporter</td>\n",
              "      <td>Rotten</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2010-02-10</td>\n",
              "      <td>What's really lacking in The Lightning Thief i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  rotten_tomatoes_link  ...                                     review_content\n",
              "0            m/0814255  ...  A fantasy adventure that fuses Greek mythology...\n",
              "1            m/0814255  ...  Uma Thurman as Medusa, the gorgon with a coiff...\n",
              "2            m/0814255  ...  With a top-notch cast and dazzling special eff...\n",
              "3            m/0814255  ...  Whether audiences will get behind The Lightnin...\n",
              "4            m/0814255  ...  What's really lacking in The Lightning Thief i...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "lctBHHlNeBuo",
        "outputId": "4fd8d7a0-04cd-4f00-897f-9db63eda29bb"
      },
      "source": [
        "# Pegando as colunas que nos interessam\n",
        "df_rt_clean = df_rt[['review_content', 'review_type']].copy()\n",
        "df_rt_clean.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_content</th>\n",
              "      <th>review_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A fantasy adventure that fuses Greek mythology...</td>\n",
              "      <td>Fresh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Uma Thurman as Medusa, the gorgon with a coiff...</td>\n",
              "      <td>Fresh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>With a top-notch cast and dazzling special eff...</td>\n",
              "      <td>Fresh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Whether audiences will get behind The Lightnin...</td>\n",
              "      <td>Fresh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What's really lacking in The Lightning Thief i...</td>\n",
              "      <td>Rotten</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      review_content review_type\n",
              "0  A fantasy adventure that fuses Greek mythology...       Fresh\n",
              "1  Uma Thurman as Medusa, the gorgon with a coiff...       Fresh\n",
              "2  With a top-notch cast and dazzling special eff...       Fresh\n",
              "3  Whether audiences will get behind The Lightnin...       Fresh\n",
              "4  What's really lacking in The Lightning Thief i...      Rotten"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iawgvcs9fIVo",
        "outputId": "f339621e-8078-4f9b-ae0f-778f492277f1"
      },
      "source": [
        "# Checando e removendo os valores nulos\n",
        "print(df_rt_clean['review_content'].isna().sum())\n",
        "df_rt_clean.dropna(inplace=True)\n",
        "print(df_rt_clean['review_content'].isna().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65806\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24kaiqlVfh5Z",
        "outputId": "a6e7e227-9da4-4da2-e127-3a79eca6022d"
      },
      "source": [
        "# Checando e removendo os valores duplicados\n",
        "print(df_rt_clean.duplicated(subset=['review_content']).sum())\n",
        "df_rt_clean.drop_duplicates(inplace=True, ignore_index=True)\n",
        "print(df_rt_clean.duplicated(subset=['review_content']).sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "115030\n",
            "75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMYbjzW6g8te"
      },
      "source": [
        "# Colocando o mesmo nome das classes\n",
        "df_rt_clean['review_type'].replace('Fresh', 'positive', inplace=True)\n",
        "df_rt_clean['review_type'].replace('Rotten', 'negative', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FgUjwXPhHwt",
        "outputId": "b5a184b7-d4ad-463f-8d94-03aa1ad93dec"
      },
      "source": [
        "df_rt_clean.review_type.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    607388\n",
              "negative    341868\n",
              "Name: review_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THJ6ep5kvLEO"
      },
      "source": [
        "# BALANCEAMENTO USANDO UNDERSAMPLING\n",
        "bln_df_rt_clean = df_rt_clean.groupby('review_type')\n",
        "bln_df_rt_clean = pd.DataFrame(bln_df_rt_clean.apply(lambda x: x.sample(bln_df_rt_clean.size().min()).reset_index(drop=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC3bGuR9vwIV",
        "outputId": "f0a1aaa1-2441-49c4-f07f-2d7a2d0a6b1b"
      },
      "source": [
        "bln_df_rt_clean.review_type.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    341868\n",
              "negative    341868\n",
              "Name: review_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8nL0n0FhnVi",
        "outputId": "3d0b3f61-6e39-407e-e333-03403c35edea"
      },
      "source": [
        "# Pegando uma amostra\n",
        "sample_df_rt = bln_df_rt_clean.sample(10000)\n",
        "sample_df_rt.review_type.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    5031\n",
              "negative    4969\n",
              "Name: review_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "5_koApE1qnBy",
        "outputId": "21b62d51-025e-4f84-fc5c-aab06bfcd173"
      },
      "source": [
        "# Transformando as frases do Rotten Tomatoes, assim como fizemos com o dataset original\n",
        "sample_df_rt_proc = transform_text(sample_df_rt, 'review_content', 'review_type')\n",
        "sample_df_rt_proc.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>review_content</th>\n",
              "      <th>review_type</th>\n",
              "      <th>cleaner</th>\n",
              "      <th>cleaner_str</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>review_type</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <th>78412</th>\n",
              "      <td>An exceptionally well-executed and emotionally...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[except, emot, heart, wrench, documentari]</td>\n",
              "      <td>except emot heart wrench documentari</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">negative</th>\n",
              "      <th>64151</th>\n",
              "      <td>Striking Distance is an exhausted reassembly o...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[strike, distanc, exhaust, reassembl, bit, pie...</td>\n",
              "      <td>strike distanc exhaust reassembl bit piec movi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204799</th>\n",
              "      <td>With the exception of ramming my head into a b...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[except, ram, head, brick, wall, back, repeat,...</td>\n",
              "      <td>except ram head brick wall back repeat exercis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <th>37757</th>\n",
              "      <td>Hopefully, The Hate U Give will spark conversa...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[hope, hate, give, spark, convers, around, cou...</td>\n",
              "      <td>hope hate give spark convers around countri in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <th>132059</th>\n",
              "      <td>Isn't involving enough to sustain interest whe...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[involv, enough, sustain, interest, geni, offs...</td>\n",
              "      <td>involv enough sustain interest geni offscreen</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       review_content  ...                                        cleaner_str\n",
              "review_type                                                            ...                                                   \n",
              "positive    78412   An exceptionally well-executed and emotionally...  ...               except emot heart wrench documentari\n",
              "negative    64151   Striking Distance is an exhausted reassembly o...  ...  strike distanc exhaust reassembl bit piec movi...\n",
              "            204799  With the exception of ramming my head into a b...  ...  except ram head brick wall back repeat exercis...\n",
              "positive    37757   Hopefully, The Hate U Give will spark conversa...  ...  hope hate give spark convers around countri in...\n",
              "negative    132059  Isn't involving enough to sustain interest whe...  ...      involv enough sustain interest geni offscreen\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzdsPZg-rPs_"
      },
      "source": [
        "# Vetorizando o input e pegando o output esperado\n",
        "rt_X = vectorizer3.transform(sample_df_rt_proc['cleaner_str'])\n",
        "rt_y = sample_df_rt_proc['review_type']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZlrr9YqrgV8",
        "outputId": "ec00fb9e-62f2-4d4d-84f0-f2f254a44fee"
      },
      "source": [
        "# Testando com os classificadores do sklearn\n",
        "test_models(rt_X, rt_y, 'Rotten Tomatoes')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing model Perceptron with Rotten Tomatoes dataset\n",
            "Accuracy: 0.6692\n",
            "Precision: 0.672557778209361\n",
            "Recall: 0.6810226155358898\n",
            "Testing model Random Forest with Rotten Tomatoes dataset\n",
            "Accuracy: 0.6153\n",
            "Precision: 0.5772012970815664\n",
            "Recall: 0.9101278269419862\n",
            "Testing model Gradient Boost with Rotten Tomatoes dataset\n",
            "Accuracy: 0.633\n",
            "Precision: 0.5949791918378305\n",
            "Recall: 0.8715830875122911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq8GBKBV6H2-"
      },
      "source": [
        "# Vetorizando o input e pegando o output esperado (2)\n",
        "rt_X2 = vectorizer4.transform(sample_df_rt_proc['cleaner_str'])\n",
        "rt_y2 = sample_df_rt_proc['review_type']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnSiE_lb6SHk",
        "outputId": "0bbf1d9a-2cd7-4296-8899-708272746b5d"
      },
      "source": [
        "# Testando com os classificadores do sklearn (2)\n",
        "test_models2(rt_X2, rt_y2, 'Rotten Tomatoes')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing model Perceptron with Rotten Tomatoes dataset\n",
            "Accuracy: 0.7019\n",
            "Precision: 0.6904496469713861\n",
            "Recall: 0.7386205525740409\n",
            "Testing model Random Forest with Rotten Tomatoes dataset\n",
            "Accuracy: 0.6326\n",
            "Precision: 0.5870429762668378\n",
            "Recall: 0.9095607235142119\n",
            "Testing model Gradient Boost with Rotten Tomatoes dataset\n",
            "Accuracy: 0.6201\n",
            "Precision: 0.5777974235918161\n",
            "Recall: 0.9093619558735838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnyJkD9Ls1Yr",
        "outputId": "72006b70-bdbe-4720-e3ed-058b9aeec789"
      },
      "source": [
        "# Calculando desempenho do BERT no Rotten Tomatoes\n",
        "rt_expected = sample_df_rt.review_type.values\n",
        "rt_predicted = predictor_load.predict(sample_df_rt.review_content.values)\n",
        "calculate_metrics(rt_expected, rt_predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8234\n",
            "Precision: 0.8159147154007234\n",
            "Recall: 0.8428711897738447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3QNcrIuZ1cp"
      },
      "source": [
        "## Amazon, yelp, imdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUgrHsTT9FUw",
        "outputId": "88095391-d181-406e-ac04-c18de14bcf93"
      },
      "source": [
        "# Dataset com reviews da amazon, yelp, imdb\n",
        "!unzip gdrive/MyDrive/sentiment_labelled_sentences.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  gdrive/MyDrive/sentiment_labelled_sentences.zip\n",
            "   creating: sentiment labelled sentences/\n",
            "  inflating: sentiment labelled sentences/amazon_cells_labelled.csv  \n",
            "  inflating: sentiment labelled sentences/amazon_cells_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/imdb_labelled.csv  \n",
            "  inflating: sentiment labelled sentences/imdb_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/readme.txt  \n",
            "  inflating: sentiment labelled sentences/yelp_labelled.csv  \n",
            "  inflating: sentiment labelled sentences/yelp_labelled.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScuDRz069550",
        "outputId": "1f3f48dc-5b8d-4d76-b3b7-a173cec79579"
      },
      "source": [
        "# Lendo e colocando tudo em um dataset\n",
        "archive_dict = {'amazon': 'sentiment labelled sentences/amazon_cells_labelled.txt',\n",
        "                 'yelp': 'sentiment labelled sentences/yelp_labelled.txt',\n",
        "                 'imdb': 'sentiment labelled sentences/imdb_labelled.txt'}\n",
        "df_list = []\n",
        "\n",
        "for source, archive in archive_dict.items():\n",
        "  df_test = pd.read_csv(archive, names=['sentence', 'label'], sep='\\t')\n",
        "  df_test['source'] = source\n",
        "  df_list.append(df_test)\n",
        "\n",
        "df_test = pd.concat(df_list)\n",
        "print(df_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2748, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb9t2TrDBVwb"
      },
      "source": [
        "df_test['label'].replace(1, 'positive', inplace=True)\n",
        "df_test['label'].replace(0, 'negative', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5BceHdX-QnM"
      },
      "source": [
        "# Separando por site\n",
        "df_amazon = df_test[df_test['source']=='amazon']\n",
        "df_yelp = df_test[df_test['source']=='yelp']\n",
        "df_imdb = df_test[df_test['source']=='imdb']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "aHQbDhYN-2A8",
        "outputId": "7013e6ca-efc3-4b1e-f1b2-d3848db5cde5"
      },
      "source": [
        "df_amazon.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here i...</td>\n",
              "      <td>negative</td>\n",
              "      <td>amazon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>positive</td>\n",
              "      <td>amazon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>positive</td>\n",
              "      <td>amazon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>negative</td>\n",
              "      <td>amazon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>positive</td>\n",
              "      <td>amazon</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence     label  source\n",
              "0  So there is no way for me to plug it in here i...  negative  amazon\n",
              "1                        Good case, Excellent value.  positive  amazon\n",
              "2                             Great for the jawbone.  positive  amazon\n",
              "3  Tied to charger for conversations lasting more...  negative  amazon\n",
              "4                                  The mic is great.  positive  amazon"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zznZTwv69yjE"
      },
      "source": [
        "### sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "R_NEWitmAYlO",
        "outputId": "e73abefe-ca89-48e4-bd4f-4d1cb03360be"
      },
      "source": [
        "# Transformando as frases da amazon, assim como fizemos com o dataset original\n",
        "df_amazon_proc = transform_text(df_amazon, 'sentence', 'label')\n",
        "df_amazon_proc.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaner</th>\n",
              "      <th>cleaner_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here i...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[way, plug, us, unless, go, convert]</td>\n",
              "      <td>way plug us unless go convert</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>positive</td>\n",
              "      <td>[good, case, excel, valu]</td>\n",
              "      <td>good case excel valu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>positive</td>\n",
              "      <td>[great, jawbon]</td>\n",
              "      <td>great jawbon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[tie, charger, convers, last, problem]</td>\n",
              "      <td>tie charger convers last problem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>positive</td>\n",
              "      <td>[mic, great]</td>\n",
              "      <td>mic great</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                       cleaner_str\n",
              "0  So there is no way for me to plug it in here i...  ...     way plug us unless go convert\n",
              "1                        Good case, Excellent value.  ...              good case excel valu\n",
              "2                             Great for the jawbone.  ...                      great jawbon\n",
              "3  Tied to charger for conversations lasting more...  ...  tie charger convers last problem\n",
              "4                                  The mic is great.  ...                         mic great\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxtN4s0LPjF8"
      },
      "source": [
        "# Vetorizando o input e pegando o output esperado\n",
        "amazon_X = vectorizer3.transform(df_amazon_proc['cleaner_str'])\n",
        "amazon_y = df_amazon_proc['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJADB54YSCBd"
      },
      "source": [
        "# Fazendo o mesmo pro yelp\n",
        "df_yelp_proc = transform_text(df_yelp, 'sentence', 'label')\n",
        "yelp_X = vectorizer3.transform(df_yelp_proc['cleaner_str'])\n",
        "yelp_y = df_yelp_proc['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIaZz5O7R1T0"
      },
      "source": [
        "# Fazendo o mesmo pro novo imdb\n",
        "df_imdb_proc = transform_text(df_imdb, 'sentence', 'label')\n",
        "imdb_X = vectorizer3.transform(df_imdb_proc['cleaner_str'])\n",
        "imdb_y = df_imdb_proc['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elPFN7MJPYsz",
        "outputId": "0b88f052-7bcd-4ea3-fdbc-7309f79935a6"
      },
      "source": [
        "test_models(amazon_X, amazon_y, 'amazon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing model Perceptron with amazon dataset\n",
            "Accuracy: 0.686\n",
            "Precision: 0.6845238095238095\n",
            "Recall: 0.69\n",
            "Testing model Random Forest with amazon dataset\n",
            "Accuracy: 0.63\n",
            "Precision: 0.5783132530120482\n",
            "Recall: 0.96\n",
            "Testing model Gradient Boost with amazon dataset\n",
            "Accuracy: 0.64\n",
            "Precision: 0.5877192982456141\n",
            "Recall: 0.938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_I3AMNFSUuo",
        "outputId": "64107f2b-5625-4f35-bbc7-f58acc096e42"
      },
      "source": [
        "test_models(yelp_X, yelp_y, 'yelp')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing model Perceptron with yelp dataset\n",
            "Accuracy: 0.702\n",
            "Precision: 0.6803571428571429\n",
            "Recall: 0.762\n",
            "Testing model Random Forest with yelp dataset\n",
            "Accuracy: 0.634\n",
            "Precision: 0.5797619047619048\n",
            "Recall: 0.974\n",
            "Testing model Gradient Boost with yelp dataset\n",
            "Accuracy: 0.631\n",
            "Precision: 0.5811648079306072\n",
            "Recall: 0.938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6taQSJNSYS7",
        "outputId": "e7d10306-fa48-48cb-861a-f38f6a30e470"
      },
      "source": [
        "test_models(imdb_X, imdb_y, 'imdb')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing model Perceptron with imdb dataset\n",
            "Accuracy: 0.7914438502673797\n",
            "Precision: 0.859375\n",
            "Recall: 0.7124352331606217\n",
            "Testing model Random Forest with imdb dataset\n",
            "Accuracy: 0.7834224598930482\n",
            "Precision: 0.7196078431372549\n",
            "Recall: 0.9507772020725389\n",
            "Testing model Gradient Boost with imdb dataset\n",
            "Accuracy: 0.7780748663101604\n",
            "Precision: 0.7235772357723578\n",
            "Recall: 0.9222797927461139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzMnAwrC7A02"
      },
      "source": [
        "# Vetorizando o input (2)\n",
        "amazon_X2 = vectorizer4.transform(df_amazon_proc['cleaner_str'])\n",
        "yelp_X2 = vectorizer4.transform(df_yelp_proc['cleaner_str'])\n",
        "imdb_X2 = vectorizer4.transform(df_imdb_proc['cleaner_str'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3Olsd7Z7oyM",
        "outputId": "e786d70d-4134-42d6-df44-cccb66e726ea"
      },
      "source": [
        "test_models2(amazon_X2, amazon_y, 'amazon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing model Perceptron with amazon dataset\n",
            "Accuracy: 0.738\n",
            "Precision: 0.7195571955719557\n",
            "Recall: 0.78\n",
            "Testing model Random Forest with amazon dataset\n",
            "Accuracy: 0.647\n",
            "Precision: 0.5931558935361216\n",
            "Recall: 0.936\n",
            "Testing model Gradient Boost with amazon dataset\n",
            "Accuracy: 0.63\n",
            "Precision: 0.5779376498800959\n",
            "Recall: 0.964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1mqWs9q7vbj",
        "outputId": "4ffc7927-ef4a-4e8f-c541-44ccf9c4924a"
      },
      "source": [
        "test_models2(yelp_X2, yelp_y, 'yelp')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing model Perceptron with yelp dataset\n",
            "Accuracy: 0.717\n",
            "Precision: 0.6848381601362862\n",
            "Recall: 0.804\n",
            "Testing model Random Forest with yelp dataset\n",
            "Accuracy: 0.673\n",
            "Precision: 0.6077210460772104\n",
            "Recall: 0.976\n",
            "Testing model Gradient Boost with yelp dataset\n",
            "Accuracy: 0.648\n",
            "Precision: 0.589588377723971\n",
            "Recall: 0.974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd-wqZuF73fJ",
        "outputId": "26a47e1a-4fc4-4195-ad28-ce9311e70cba"
      },
      "source": [
        "test_models2(imdb_X2, imdb_y, 'imdb')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing model Perceptron with imdb dataset\n",
            "Accuracy: 0.81951871657754\n",
            "Precision: 0.8515406162464986\n",
            "Recall: 0.7875647668393783\n",
            "Testing model Random Forest with imdb dataset\n",
            "Accuracy: 0.7981283422459893\n",
            "Precision: 0.7373737373737373\n",
            "Recall: 0.9455958549222798\n",
            "Testing model Gradient Boost with imdb dataset\n",
            "Accuracy: 0.7981283422459893\n",
            "Precision: 0.7317554240631163\n",
            "Recall: 0.961139896373057\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vwbOS_A9tRL"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P26iGXd_FxE3",
        "outputId": "bb6d643f-ae8c-481d-8054-832392e5e90c"
      },
      "source": [
        "# Calculando desempenho do BERT no amazon\n",
        "amazon_expected = df_amazon.label.values\n",
        "amazon_predicted = predictor_load.predict(df_amazon.sentence.values)\n",
        "calculate_metrics(amazon_expected, amazon_predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.882\n",
            "Precision: 0.9170305676855895\n",
            "Recall: 0.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB6MmQGzFNpF",
        "outputId": "a9ff2bf8-b4cc-441b-cce8-e5a09d93f773"
      },
      "source": [
        "# Calculando desempenho do BERT no yelp\n",
        "yelp_expected = df_yelp.label.values\n",
        "yelp_predicted = predictor_load.predict(df_yelp.sentence.values)\n",
        "calculate_metrics(yelp_expected, yelp_predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.889\n",
            "Precision: 0.9164882226980728\n",
            "Recall: 0.856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ii4g3zbDV__",
        "outputId": "879cb242-4fad-498a-9d5d-a8ec3e0db344"
      },
      "source": [
        "# Calculando desempenho do BERT no novo imdb\n",
        "imdb_expected = df_imdb.label.values\n",
        "imdb_predicted = predictor_load.predict(df_imdb.sentence.values)\n",
        "calculate_metrics(imdb_expected, imdb_predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9451871657754011\n",
            "Precision: 0.9624664879356568\n",
            "Recall: 0.9300518134715026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0aDHOB7cgu4"
      },
      "source": [
        "# Testando eficácia com tradução - INCOMPLETO\n",
        "Por conta do tempo achamos melhor desenvolver mais as outras partes do projeto, já que no fim esta abordagem apenas \"mediria\" o quão bom é o tradutor e o quão parecidas são as reviews em PT e EN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yu26sy_laLE"
      },
      "source": [
        "pt_reviews = ['Shrek é o melhor filme de animação que eu já vi em toda a minha vida. Mistura comédia,amizade e claro,romance. Mostra que a beleza está dentro de casa um de nós,e que todos temos um amor verdadeiro.',\n",
        "        'Resultado péssimo em relação à inovação tecnologica e roteiro sofrível. A fotografia e efeitos visuais são tão estranhos que não conseguimos nem prestar atenção nas cenas de ação. Decepção!',\n",
        "        'Projeto Gemini não aproveita o bom elenco e a ideia central atraente e aposta em uma ação morna com diálogos batidos e final previsível. O trailer é melhor que o filme.'\n",
        "        ]\n",
        "\n",
        "pt_sentimento = ['positive', 'negative', 'negative']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XxqqQwL7XIU",
        "outputId": "d8ed51cf-3ba8-4429-a589-8a44b6ad22a7"
      },
      "source": [
        "predictor_load.predict(pt_reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['positive', 'positive', 'positive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT5nPqCEWBSU"
      },
      "source": [
        "# Deu ruim na biblioteca esses dias: https://stackoverflow.com/questions/52455774/googletrans-stopped-working-with-error-nonetype-object-has-no-attribute-group\n",
        "!pip install googletrans==3.1.0a0\n",
        "#import googletrans\n",
        "from googletrans import Translator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6naxdKWlPPD"
      },
      "source": [
        "translator = Translator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG29O9CS7ze2"
      },
      "source": [
        "translations = translator.translate(pt_reviews, src='pt')\n",
        "#translation.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMOCmaV5l_PQ",
        "outputId": "511abf66-d331-43ff-f55b-4c101c5525fe"
      },
      "source": [
        "for translation in translations:\n",
        "  print(translation.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shrek is the best animated film I have ever seen in my life. Mixing comedy, friendship and of course, romance. It shows that beauty is inside one of us, and that we all have true love.\n",
            "Bad result in relation to technological innovation and poor script. The photography and visual effects are so strange that we can't even pay attention to the action scenes. Disappointment!\n",
            "Projeto Gemini does not take advantage of the good cast and attractive central idea and bets on a warm action with beaten dialogues and predictable ending. The trailer is better than the movie.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}